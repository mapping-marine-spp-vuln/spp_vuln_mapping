---
title: "Assemble grouping traits"
author: "*Compiled on `r date()` by `r Sys.info()['user']`*"
output: 
  html_document:
    code_folding: hide
    toc: true
    toc_depth: 3
    toc_float: yes
    number_sections: true
    theme: cerulean
    highlight: haddock
    includes: 
      in_header: '~/github/src/templates/ohara_hdr.html'
  pdf_document:
    toc: true
---

``` {r setup, echo = TRUE, message = FALSE, warning = FALSE}

knitr::opts_chunk$set(echo = TRUE, message = FALSE, warning = FALSE)

library(oharac)
oharac::setup()
library(cluster)
library(factoextra)
library(ggfortify)
source(here('fb_slb_fxns.R'))


```

# Summary

Gather processed traits data and assemble into a dataframe.  Apply a clustering algorithm to identify groupings.

# Data

See individual trait scripts.

# Methods

## Assemble dataframe

```{r}
trait_fs <- list.files(here('_data/traits_grouping'), full.names = TRUE)
trait_fs <- trait_fs[!str_detect(trait_fs, 'raw.csv')]

df <- lapply(trait_fs, read_csv) %>%
  bind_rows()
```

Let's limit to observations with mean gapfill of rank 3 (family) or lower.  For now this gets us 17k observations.  Opening that up to rank 4 (order) or lower gets us 64k observations.

```{r}
df_clean <- df %>%
  filter(gf_level <= 3) %>%
  select(-sd, -nspp) %>% 
  group_by(species) %>%
  mutate(gf_level = mean(gf_level),
         n_traits = n()) %>%
  ungroup() %>%
  distinct() %>%
  spread(trait, value) %>%
  drop_na() %>%
  left_join(get_worms()) %>%
  arrange(class)

n_spp_clean <- df_clean$species %>%
  n_distinct()

df_rescale <- df_clean %>%
  select(-gf_level, -n_traits, -troph_se) %>%
  select(where(is.numeric)) %>%
  scale()
```

``` {r pca}
traits_pca <- df_rescale %>%
  prcomp()
autoplot(traits_pca,
         data = df_clean,
         loadings = TRUE,
         colour = 'class',
         loadings.label = TRUE,
         loadings.colour = "black",
         loadings.label.colour = "black",
         loadings.label.vjust = -0.5
         ) +
  scale_color_viridis_d() +
  theme_minimal()

# Variance explained by each PC
screeplot(traits_pca, type = "lines")

# See the loadings (weighting for each principal component)
traits_pca$rotation %>% round(3)
```

### identify an appropriate number of clusters

#### Knee method

Iterate over different values of $k$ and identify a "knee".
``` {r knee method, cache = TRUE}
set.seed(42)
wss <- function(k, df) {
  kmeans(df, k, nstart = 10)$tot.withinss
}

# Compute and plot wss for k = 1 to k = 15
k_vec <- 1:15

# extract wss for 2-15 clusters
wss_values <- sapply(k_vec, wss, df = df_rescale)

plot(k_vec, wss_values,
     type="b", pch = 19, frame = FALSE, 
     xlab="Number of clusters K",
     ylab="Total within-clusters sum of squares")

plot(k_vec[6:15], wss_values[6:15],
     type="b", pch = 19, frame = FALSE, 
     xlab="Number of clusters K",
     ylab="Total within-clusters sum of squares")

```

From this method, we see a knee around 6 looking at the overall set, and zooming in, 9 or higher appears optimal. 

#### Silhouette method 

From https://uc-r.github.io/kmeans_clustering:

> In short, the average silhouette approach measures the quality of a clustering. That is, it determines how well each object lies within its cluster. A high average silhouette width indicates a good clustering. The average silhouette method computes the average silhouette of observations for different values of _k_. The optimal number of clusters _k_ is the one that maximizes the average silhouette over a range of possible values for _k_.

```{r silhouette method, cache = TRUE}
set.seed(42)
avg_sil <- function(k, df) {
  km_res <- kmeans(df, centers = k, nstart = 4)
  ss <- silhouette(km_res$cluster, dist(df))
  mean(ss[, 3])
}

# Compute and plot wss for k = 2 to k = 15
k_vec <- 2:15

# extract avg silhouette for 2-15 clusters
avg_sil_values <- sapply(k_vec, avg_sil, df = df_rescale)

plot(k_vec, avg_sil_values,
     type = "b", pch = 19, frame = FALSE, 
     xlab = "Number of clusters K",
     ylab = "Average Silhouettes")

# Similar to the elbow method, this process to compute the “average 
# silhoutte method” has been wrapped up in a single function (fviz_nbclust):

fviz_nbclust(df_rescale, kmeans, k.max = 15, method = "silhouette")
```

From this method it appears that 11 is an optimal number of clusters (and 8 is locally suboptimal!), though many values from 9-14 all look pretty good.

#### Gap Statistic Method

From https://uc-r.github.io/kmeans_clustering: 

> The gap statistic has been published by R. Tibshirani, G. Walther, and T. Hastie (Standford University, 2001). The approach can be applied to any clustering method (i.e. K-means clustering, hierarchical clustering). The gap statistic compares the total intracluster variation for different values of k with their expected values under null reference distribution of the data (i.e. a distribution with no obvious clustering). The reference dataset is generated using Monte Carlo simulations of the sampling process. That is, for each variable
$x_i$ in the data set we compute its range $[\min(x_i), \max(x_i)]$ and generate values for the $n$ points uniformly from the interval min to max.

> For the observed data and the the reference data, the total intracluster variation is computed using different values of $k$. The gap statistic for a given $k$ is defined as follow:

$$Gap_n(k) = E^*_n \log(W_k) - \log(W_k)$$
>where $E^*_n$ denotes the expectation under a sample size $n$ from the reference distribution. $E^*_n$ is defined via bootstrapping (B) by generating B copies of the reference datasets and, by computing the average $\log(W^*_k)$. The gap statistic measures the deviation of the observed $W_k$ value from its expected value under the null hypothesis. The estimate of the optimal clusters ($\hat{k}$) will be the value that maximizes $Gap_n(k)$. This means that the clustering structure is far away from the uniform distribution of points.

```{r gap stat method, cache = TRUE}
set.seed(42)
gap_stat <- clusGap(df_rescale, verbose = FALSE,
                    FUN = kmeans, nstart = 25,
                    K.max = 15, B = 50)
# Print the result
print(gap_stat, method = "firstmax")

fviz_gap_stat(gap_stat)
```

This method seems to indicate that more clusters is better.  However (based on a sample version), it looks like beyond 9, the marginal value of additional bins levels off...

``` {r distance matrix, eval = FALSE}
rowsamps <- sample(1:nrow(df_rescale), 500, replace = FALSE)
d_mtx_samp <- get_dist(df_rescale[rowsamps, ])
d_mtx <- get_dist(df_rescale)
fviz_dist(d_mtx_samp, 
          gradient = list(low = "#00AFBB", mid = "white", high = "#FC4E07"),
          show_labels = FALSE)

```

### Running K-means with optimal $k$

It appears that a k between 6-10 is reasonable.  Let's choose 8.  Identify clusters across all species and then examine cluster breakdown by class.  Compare `pam()` and `kmeans()`.

``` {r k means}
system.time({
  k8 <- kmeans(df_rescale, centers = 8, nstart = 25)
}) ### ~ 2 seconds for 25 starts, 64k observations

system.time({
  p8 <- pam(df_rescale, 
            k = 8,
            nstart = 1,
            variant = 'faster')
  ### note other variants like 'original', 'o_1', ... are probably better choices
}) 
### - with 17k rows on 'faster' with 1 nstart - 11 seconds
### looks like it scales > n^2 
### on 'original' variant, 2000 rows ~ 2.7 sec for 1 nstart, 4000 rows ~ 18.5 sec
### note pam function can't use more than 65536 observations...
### MUCH slower than kmeans (though more robust apparently)

k_vs_p <- data.frame(k = k8$cluster,
                     p = p8$clustering) %>%
  group_by(k, p) %>%
  mutate(n = n()) %>%
  ungroup() %>%
  arrange(desc(n)) %>%
  mutate(k_fct = fct_inorder(as.character(k)),
         p_fct = fct_inorder(as.character(p)))
ggplot(k_vs_p, aes(x = k_fct, y = p_fct)) +
  geom_jitter()

fviz_cluster(k8, data = df_rescale, labelsize = NA) +
  labs(title = 'kmeans')
fviz_cluster(p8, data = df_rescale, labelsize = NA) +
  labs(title = 'pam')

df_clusters <- df_clean %>%
  mutate(k_clust = k8$cluster,
         p_clust = p8$cluster)
```

``` {r}
clusters_by_class <- df_clusters %>%
  group_by(class, p_clust) %>%
  summarize(nspp = n_distinct(species)) %>%
  group_by(class) %>%
  mutate(ntot = sum(nspp),
         pct_in_cluster = nspp / ntot,
         cluster_char = as.character(p_clust),
         ntot_lbl = ifelse(p_clust == first(p_clust), ntot, NA)) %>%
  ungroup()

ggplot(clusters_by_class, aes(x = class, y = pct_in_cluster, fill = cluster_char)) +
  geom_col() +
  geom_text(y = 1.01, aes(label = ntot_lbl), hjust = 0) +
  scale_fill_brewer(palette = 'Dark2') +
  coord_flip() +
  scale_y_continuous(limits = c(0, 1.1), breaks = seq(0, 1, .25)) +
  theme_minimal() +
  theme(axis.title.y = element_blank(),
        panel.grid.major = element_blank()) +
  labs(fill = 'cluster', y = 'Percent of cluster')
```

``` {r}
classes_by_cluster <- df_clusters %>%
  group_by(class, p_clust) %>%
  summarize(nspp = n_distinct(species)) %>%
  group_by(p_clust) %>%
  mutate(ntot = sum(nspp),
         pct_in_class = nspp / ntot,
         ntot_lbl = ifelse(class == first(class), ntot, NA),
         p_clust = as.character(p_clust)) %>%
  ungroup()

n_class <- classes_by_cluster$class %>% n_distinct()
cols <- c(hcl.colors(5, palette = 'Reds'), hcl.colors(8, palette = 'Viridis'))[sample(1:13,13)]

ggplot(classes_by_cluster, aes(x = p_clust, y = pct_in_class, fill = class)) +
  geom_col() +
  geom_text(y = 1.01, aes(label = ntot_lbl), hjust = 0) +
  scale_fill_manual(values = cols) +
  coord_flip() +
  scale_y_continuous(limits = c(0, 1.1), breaks = seq(0, 1, .25)) +
  theme_minimal() +
  theme(axis.title.y = element_blank(),
        panel.grid.major = element_blank()) +
  labs(fill = 'cluster', y = 'Percent of cluster')
```

## Examine trait centers by cluster

```{r}
centers_df <- data.frame(cluster = 1:8, scale(k8$centers), scale(p8$medoids)) %>%
  gather(trait, center, -cluster) %>%
  mutate(cluster = as.character(cluster),
         method = ifelse(str_detect(trait, '\\.1$'), 'p', 'k'),
         trait = str_replace(trait, '\\.1$', ''))

ggplot(centers_df, aes(x = cluster, y = center, color = method)) +
  geom_hline(yintercept = 0, size = .25, color = 'darkred') +
  geom_point() +
  facet_wrap(~trait, ncol = 4) +
  labs(title = 'trait/cluster centers by trait')
ggplot(centers_df, aes(x = trait, y = center, color = method)) +
  geom_hline(yintercept = 0, size = .25, color = 'darkred') +
  geom_point() +
  facet_wrap(~cluster, ncol = 4) +
  theme(axis.text = element_text(angle = 70, hjust = 1)) +
  labs(title = 'trait/cluster centers by cluster')
```

