---
title: "Examine PAM clustering methods for functional grouping"
author: "*Compiled on `r date()` by `r Sys.info()['user']`*"
output: 
  html_document:
    code_folding: hide
    toc: true
    toc_depth: 3
    toc_float: yes
    number_sections: true
    theme: cerulean
    highlight: haddock
    includes: 
      in_header: '~/github/src/templates/ohara_hdr.html'
  pdf_document:
    toc: true
---

``` {r setup, echo = TRUE, message = FALSE, warning = FALSE}

knitr::opts_chunk$set(echo = TRUE, message = FALSE, warning = FALSE)

library(oharac)
oharac::setup()
library(randomForest)
library(cluster)
library(caret)
library(factoextra)
library(ggfortify)
source(here('fb_slb_fxns.R'))


```

# Summary

Gather processed traits data and assemble into a dataframe.  Apply a clustering algorithm to identify groupings: first, use Random Forest to create a dissimilarity matrix, then use Hierarchical clustering to identify groups of similar species.

# Data

See individual trait scripts.

# Methods

## Assemble dataframe

Let's limit to observations with mean gapfill of rank 3 (family) or lower.  For now this gets us 17k observations.  Limiting that to rank 4 (order) or lower gets us 64k observations, or perhaps it is more efficient to gapfill groups upwards?  For Random Forest, use all traits in their rawest form (though there is some gapfilling already occurring in the trait prep scripts).

```{r assemble raw traits}
gf_cut <- 3

trait_length <- read_csv(here('_data/traits_grouping/trait_length.csv')) %>%
  filter(gf_level <= gf_cut) %>%
  select(species, log_l = value)
trait_reprod <- read_csv(here('_data/traits_grouping/trait_reproduction_raw.csv')) %>%
  filter(gf_level <= gf_cut) %>%
  select(species, depend_score, log_gen_time, log_max_fecund, pld_score, protect_score)
trait_depth  <- read_csv(here('_data/traits_grouping/trait_depth_raw.csv')) %>%
  filter(gf_level <= gf_cut) %>%
  select(species, depth, terr_score, vert_mig_score)
trait_mobil  <- read_csv(here('_data/traits_grouping/trait_mobility.csv')) %>%
  filter(gf_level <= gf_cut) %>%
  select(species, mob_score = value)
trait_troph  <- read_csv(here('_data/traits_grouping/trait_trophic_level.csv')) %>%
  filter(gf_level <= gf_cut) %>%
  select(species, troph_level = value)
traits_df <- trait_length %>%
  full_join(trait_reprod) %>%
  full_join(trait_depth) %>%
  full_join(trait_mobil) %>%
  full_join(trait_troph)
```

```{r attach taxa}
taxa <- read_csv(here('_data/vuln_data/vuln_gapfilled_tx.csv')) %>%
  select(class:species) 
  
traits_dropna <- traits_df %>%
  left_join(taxa, by = 'species') %>%
  drop_na()
### leaves only 17008 observations...
```

## Unsupervised Random Forest

Let's apply the Random Forest process to the traits dataframe, without supervision, to create a proximity matrix.  Drop some low-importance variables.

```{r unsup_rf}
### n = 1000, t = 3.663; n = 2000, t = 10.096; n = 17008, t = 477.686
system.time({
  unsup_rf <- randomForest(x = traits_dropna %>% 
                             ### drop three least important vars
                             select(-terr_score, -vert_mig_score, -depend_score) %>%
                             # sample_n(2000) %>%
                             select(where(is.numeric)),
                           mtry = 3, ntree = 1000, proximity = TRUE)
})
unsup_rf
varImpPlot(unsup_rf)
```

For the subset of species with mean gapfill level of 3 or less, the following traits score low on MeanDecreaseGini and can be perhaps dropped:

* depth (~900)
* depend_score (~450) (dropped)
* vert_mig_score (~100) (dropped)
* terr_score (~100) (dropped)


## Partitioning Across Medioids

### Notes on clustering techniques:

With the results of the unsupervised Random Forest, we tried clustering in two ways: Partitioning Around Medioids (which seems standard for Random Forest examples), and Hierarchical clustering (which also appears in some Random Forest examples).  For these we identified a reasonable number of clusters and compared results; the most similar clusters scored about 80%  on Jaccard similarity index.

### PAM vs K-Means

#### K-means?

K-means does not seem appropriate for RF proximity/dissimilarity, since it relies on euclidean distances rather than a dissimilarity matrix.

#### PAM?

From `pam()` documentation: 

>For large datasets, `pam` may need too much memory or too much computation time since both are $O(n^2)$. Then, `clara()` is preferable, see its documentation.

However, the `clara()` function looks like it wants a data matrix/frame with columns for variables, rather than a dissimilarity/distance matrix.

Some arguments to `pam()` sped up the clustering process by orders of magnitude, especially `do.swap = FALSE` and `pamonce = 6`.

### identify an appropriate number of clusters

PAM requires defining a set number of clusters, though the optimal number of clusters is unknown.  The silhouette method seems standard to identify an appropriate number of clusters, and applicable across different clustering methods (counterexample, the gap statistic method seems to require Euclidean distances, which Random Forest does not provide(?))

We can use the Silhouette method for examining the relationship between number of clusters and average silhouette width.

From https://uc-r.github.io/kmeans_clustering:

> In short, the average silhouette approach measures the quality of a clustering. That is, it determines how well each object lies within its cluster. A high average silhouette width indicates a good clustering. The average silhouette method computes the average silhouette of observations for different values of _k_. The optimal number of clusters _k_ is the one that maximizes the average silhouette over a range of possible values for _k_.

```{r silhouette functions}
avg_sil_pam <- function(k, DM) {
  ### k <- 6
  system.time({
  pam_res <- pam(DM, diss = TRUE,
                 k = k, nstart = 25,
                 do.swap = FALSE, cluster.only = TRUE,
                 keep.diss = FALSE, keep.data = FALSE,
                   ### these settings are for much faster processing
                 pamonce = 6)
  })
  ss <- silhouette(pam_res, DM)
  mean_sil_w <- mean(ss[, 'sil_width'])
}

loop_over_pams <- function(k_vec = 6:50, DM) {
  message('Calculating pam silhouette for ', nrow(traits_dropna), 
          ' observations (gapfill = ', gf_cut, ')')
    
  avg_sil <- parallel::mclapply(k_vec, 
                                FUN = avg_sil_pam, 
                                DM = DM,
                                mc.cores = 8)
  avg_sil_val_vec <- unlist(avg_sil)
  
  silh_df <- data.frame(k = k_vec, silh = avg_sil_val_vec)
  return(silh_df)
}
```

```{r silhouette method hclust}
### generate a dissimilarity matrix from the unsupervised random forest output.
prox_mtx <- unsup_rf$proximity
dissim_mtx <- 1 - prox_mtx
dist_mtx <- as.dist(dissim_mtx, diag = TRUE, upper = TRUE)

silh_df <- loop_over_pams(DM = dist_mtx)

ggplot(silh_df, aes(x = k, y = silh)) +
  geom_point() +
  labs(x = "Number of clusters K",
       y = "Average Silhouette (PAM)") +
  ylim(c(0, NA)) +
  theme_minimal()

ggsave('figs/silhouette_plot_pam.png')
```

#### Plot of silhouettes

`r knitr::include_graphics('figs/silhouette_plot_pam.png')`

 