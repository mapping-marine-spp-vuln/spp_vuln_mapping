---
title: "Map biomass removal impacts"
author: "*Compiled on `r date()` by `r Sys.info()['user']`*"
output: 
  html_document:
    code_folding: hide
    toc: true
    toc_depth: 3
    toc_float: yes
    number_sections: true
    theme: cerulean
    highlight: haddock
    includes: 
      in_header: '~/github/src/templates/ohara_hdr.html'
  pdf_document:
    toc: true
---

``` {r setup, echo = TRUE, message = FALSE, warning = FALSE}

knitr::opts_chunk$set(echo = TRUE, message = FALSE, warning = FALSE, fig.height = 4, fig.width = 7)

library(terra)
library(oharac)
library(data.table)
library(tidyverse)
library(here)
source(here('common_fxns.R'))

```

# Summary

Impacts from targeted biomass removal (as opposed to bycatch) are based on fishing pressure on the targeted species (NPP-weighted catch, scripts 4a-4d in `_setup/stressors`) and the species' vulnerability, based on general adaptive capacity.  Because fishing pressure is specific to each species, we can't merely apply a mean vulnerability and pressure score.  The current script reads in the fishing stressor map for each species, multiplies by the spp vulnerability (primarily driven by adaptive capacity) to calculate impact.  These spp-level impacts are then aggregated by:

* unweighted mean impact in each cell.
* functional-vulnerability-weighted mean impact in each cell.

# Data

* Vulnerability data from Ecosphere trait-based vulnerability
* Stressor data from the script noted above - based on Watson catch data, normalized by NPP

# Methods

## Set up spp map source and vulnerability data

Filter to just biomass removal stressor.

```{r assemble spp info dataframe}
spp_info_df <- assemble_spp_info_df(fe_only = TRUE) %>%
  rename(vulnerability = stressor) %>%
  filter(vulnerability == 'biomass_removal')
```

```{r}
### These are created in _setup/stressors/4d_targeted_fishing_rescale.Rmd
catch_map_stem <- here_anx('stressors/fishing/4_rescaled_catch_by_spp_cell', 
                           '%s_spp_rescaled_catch_%s.csv')

range_mapfile_df <- spp_info_df %>%
  select(species, src, range_map_f = map_f) %>%
  distinct()
catch_mapfile_df <- spp_info_df %>%
  mutate(catch_map_f = sprintf(catch_map_stem, src, id)) %>%
  select(species, src, catch_map_f) %>%
  filter(file.exists(catch_map_f)) %>%
  distinct()

```

## Impacts by unweighted mean vulnerability

### Calculate mean biomass removal impacts per taxon

Loop over each taxon; pull all max temp stressor files for that taxon.  For each species in the taxon, multiply biomass removal stressor map by the spp vulnerability to identify impact map for that species.  Summarize across the entire taxon to mean, sd, and nspp.

```{r}
collect_catch_maps <- function(tx_catch_mapfiles) {
  if(nrow(tx_catch_mapfiles) == 0) {
    message('No catch for this taxon!')
    return(NULL)
  }
    
  ### read in all biomass removal stressor maps for this taxon - 
  message('Loading biomass removal stress maps for taxon ', t, '...')
  tx_catch_maps_list <- parallel::mclapply(
                    tx_catch_mapfiles$catch_map_f, ### f <- tx_catch_mapfiles$catch_map_f[27]
                    mc.cores = 40,
                    FUN = function(f) {
                      df <- data.table::fread(f) 
                      if(nrow(df) > 0) {
                        df <- df %>% 
                          mutate(rescaled_catch = as.numeric(rescaled_catch))
                      }
                      return(df)
                    })

  if(check_tryerror(tx_catch_maps_list)) {
    message('Something went wrong with loading maps for taxon ', t, '!')
  }
  message('Binding biomass removal stress maps for taxon ', t, '...')
  tx_catch_maps <- tx_catch_maps_list %>%
    setNames(tx_catch_mapfiles$species) %>%
    data.table::rbindlist(idcol = 'species')
  
  return(tx_catch_maps)
}

```

```{r unweighted mean}
taxa <- spp_info_df$taxon %>% unique() %>% sort()

out_stem <- here_anx('impact_maps_by_taxon/impacts_tx_biomass_removal', 
                     'imp_unwt_biomass_removal_%s_%s.tif')
### zxcv <- list.files(dirname(out_stem), pattern = 'imp_unwt', full.names = TRUE)
### unlink(zxcv)
for(t in taxa) {
  ### t <- taxa[4]
  tx_imp_df <- spp_info_df %>%
    filter(taxon == t) %>%
    select(species, v_score) %>%
    distinct()

  outf_mean <- sprintf(out_stem, t, 'mean')
  outf_sdev <- sprintf(out_stem, t, 'sdev')
  outf_nspp <- sprintf(out_stem, t, 'nspp')
  if(all(file.exists(outf_mean, outf_sdev))) {
    message('Rasters exist for taxon ', t, ' for biomass removal stressor... skipping!')
    next()
  }
  ### read in all biomass removal stressor maps for this taxon - 
  tx_catch_mapfiles <- catch_mapfile_df %>%
    filter(species %in% tx_imp_df$species) %>%
    distinct()    
  tx_catch_maps <- collect_catch_maps(tx_catch_mapfiles)
  message('Taxon ', t, ' biomass removal stressor dataframe: ', nrow(tx_catch_maps), 
          ' catch observations for ', nrow(tx_catch_mapfiles), ' species...')
  
  ### read in all spp rangemaps for this taxon
  tx_range_mapfiles <- range_mapfile_df %>%
    filter(species %in% tx_imp_df$species) %>%
    distinct()    
    
  tx_range_maps <- collect_spp_rangemaps(spp_vec  = tx_range_mapfiles$species, 
                                         file_vec = tx_range_mapfiles$range_map_f)
  ### tx_catch_maps %>% arrange(-rescaled_catch) %>% head(20)
  
  message('Taxon ', t, ' range map dataframe: ', nrow(tx_range_maps), 
          ' catch observations for ', nrow(tx_range_mapfiles), ' species...')
  
  if(is.null(tx_catch_maps)) {
    ### no catch for this taxon
    tx_catch_range_maps <- tx_range_maps %>%
      mutate(rescaled_catch = 0)
  } else {
    tx_catch_range_maps <- tx_range_maps %>%
      oharac::dt_join(tx_catch_maps, by = c('species', 'cell_id'), type = 'left') %>%
      mutate(rescaled_catch = ifelse(is.na(rescaled_catch), 0, rescaled_catch))
  }
  message('Processing mean/sd vulnerability by species in taxon ', t, ' to biomass removal stressor...')

  ### because failures might occur with summarizing a huge dataset,
  ### let's break this into chunks by cell_id - there are 6.6e+06 cells total
  chunk_size <- 500000
  n_chunks <- ceiling(6.5e6 / chunk_size)
  n_cores <- max(1, floor(n_chunks / ceiling(nrow(tx_catch_maps)/3e7)))
  # system.time({
  result_list <- parallel::mclapply(1:n_chunks, mc.cores = n_cores,
                 FUN = function(n) { ###  n <- 2
                   cell_id_min <- as.integer((n - 1) * chunk_size + 1)
                   cell_id_max <- as.integer(n * chunk_size)
                   message('Summarizing biomass removal stressor on taxon ', t, 
                           ': cells ', cell_id_min, ' - ', cell_id_max, '...')

                   chunk_sum <- tx_catch_range_maps %>%
                     filter(between(cell_id, cell_id_min, cell_id_max)) %>%
                     oharac::dt_join(tx_imp_df, by = 'species', type = 'inner') %>%
                     data.table() %>%
                     .[ , ':='(impact = v_score * rescaled_catch)] %>%
                     .[ , .(impact_mean = mean(impact),
                            impact_sd   = sd(impact),
                            n_spp       = n_distinct(species)),
                        by = .(cell_id)]
                   }) 

  if(check_tryerror(result_list)) {
    message('Something went wrong with calculations for taxon ', t, '... skipping for now!')
    next()
  }
  
  message('Binding results for taxon ', t, '...')
  result_df <- result_list %>%
    data.table::rbindlist() %>%
    filter(!is.na(cell_id))
  
  message('Creating and saving rasters for taxon ', t, '...')
  rast_mean <- map_to_mol(result_df, which = 'impact_mean')
  rast_sd   <- map_to_mol(result_df, which = 'impact_sd')
  rast_nspp <- map_to_mol(result_df, which = 'n_spp')

  writeRaster(rast_mean, outf_mean, overwrite = TRUE)
  writeRaster(rast_sd,   outf_sdev, overwrite = TRUE)
  writeRaster(rast_nspp, outf_nspp, overwrite = TRUE)
}

```


### Summarize mean biomass removal impacts across all taxa

Combine taxon-level maps using nspp-weighted mean and pooled variance.  

```{r moar helper fxns}
combine_taxa_maps <- function(tx_sst_map_df) {
  
  mean_fs <- tx_sst_map_df %>% filter(p == 'mean') %>% .$f
  sdev_fs <- tx_sst_map_df %>% filter(p == 'sdev') %>% .$f
  nspp_fs <- tx_sst_map_df %>% filter(p == 'nspp') %>% .$f
  taxa <- tx_sst_map_df$t %>% unique()
  
  message('... loading mean maps across taxa for biomass removal...')
  mean_df <- parallel::mclapply(mean_fs, mc.cores = 24, FUN = r_to_df) %>%
    setNames(taxa) %>%
    data.table::rbindlist(idcol = 'taxon') %>%
    rename(imp_mean = val)
  
  message('... loading std dev maps across taxa for biomass removal...')
  sdev_df <- parallel::mclapply(sdev_fs, mc.cores = 24, FUN = r_to_df) %>%
    setNames(taxa) %>%
    data.table::rbindlist(idcol = 'taxon') %>%
    rename(imp_sdev = val)
  
  message('... loading nspp maps across taxa for biomass removal...')
  nspp_df <- parallel::mclapply(nspp_fs, mc.cores = 24, FUN = r_to_df) %>%
    setNames(taxa) %>%
    data.table::rbindlist(idcol = 'taxon') %>%
    rename(imp_nspp = val)
  
  message('... joining mean, sd, nspp into big-ass dataframe for biomass removal...')
  big_df <- mean_df %>%
    oharac::dt_join(sdev_df, by = c('taxon', 'cell_id'), type = 'full') %>%
    oharac::dt_join(nspp_df, by = c('taxon', 'cell_id'), type = 'full')
  
  return(big_df)
}

process_mean_rasts <- function(big_df) {
  ### Set up for parallel processing
  cell_id_vec <- big_df$cell_id %>% unique()
  n_gps <- 25
  gp_vec <- rep(1:n_gps, length.out = length(cell_id_vec))
  
  ### perform parallel processing
  spp_mean_list <- parallel::mclapply(
    X = 1:n_gps, mc.cores = 25, 
    FUN = function(gp) { ### gp <- 19
      gp_cells <- cell_id_vec[gp_vec == gp]
      message('...processing ', length(gp_cells), ' cells in group ', gp, '...')
      gp_out <- big_df %>%
        filter(cell_id %in% gp_cells) %>%
        data.table() %>%
        .[ , .(imp_mean = (sum(imp_mean * imp_nspp) / sum(imp_nspp))), 
           by = .(cell_id)]
    })
  
  ### gather results
  spp_mean_df <- data.table::rbindlist(spp_mean_list)
  return(spp_mean_df)
}

process_sdev_rasts <- function(big_df) {
  ### Set up for parallel processing
  cell_id_vec <- big_df$cell_id %>% unique()
  n_gps <- 100
  gp_vec <- rep(1:n_gps, length.out = length(cell_id_vec))
  
  ### perform parallel processing
  all_spp_sdev_list <- parallel::mclapply(
    X = 1:n_gps, mc.cores = 34, 
    FUN = function(gp) { ### gp <- round(n_gps / 2)
      gp_cells <- cell_id_vec[gp_vec == gp]
      message('...processing ', length(gp_cells), ' cells in group ', gp, ' of ', n_gps, '...')
      # system.time({
        gp_sdev_out <- big_df %>%
          filter(cell_id %in% gp_cells) %>%
          data.table() %>%
          .[ , .(imp_sdev = iterated_pooled_var(imp_mean, imp_sdev, imp_nspp) %>% sqrt()),
             by = .(cell_id)]
      # })
      return(gp_sdev_out)
    })
  
  ### gather results
  all_spp_sdev <- data.table::rbindlist(all_spp_sdev_list)
  return(all_spp_sdev)
}

r_to_df <- function(f) {
  r <- terra::rast(f)
  df <- data.frame(val = as.vector(values(r)),
                   cell_id = 1:ncell(r)) %>%
    filter(!is.na(val))
  return(df)
}
```

```{r assemble taxon impact maps to total maps}

tx_bio_rem_map_df <- data.frame(f = list.files(dirname(out_stem), 
                                               pattern = 'imp_unwt_biomass_removal',
                                               full.names = TRUE)) %>%
  mutate(t = str_extract(basename(f), paste0(taxa, collapse = '|')),
         p = str_extract(basename(f), '_mean|_sdev|_nspp') %>% str_remove('_'))

impact_map_stem <- here_anx('_output/impact_maps/impact_maps_species', 
                        'impact_spp_biomass_removal_%s.tif')
  ### %s is parameter (mean, sd, nspp)

### check if total stressor maps are complete
impact_f_mean <- sprintf(impact_map_stem, 'mean')
impact_f_sdev <- sprintf(impact_map_stem, 'sdev')

if(any(!file.exists(impact_f_mean, impact_f_sdev))) {
  ### Combine mean, sdev, and nspp maps by taxon into one big dataframe
  message('Processing unweighted mean, sd, nspp maps for biomass removal stressor...')
  big_df <- combine_taxa_maps(tx_bio_rem_map_df)
  # rast_nspp_all <- raster(here('_output/nspp_maps/nspp_in_unwt_vuln.tif'))
  # catch_spp_nspp <- big_df %>%
  #   data.table() %>%
  #   .[ , .(imp_nspp = sum(imp_nspp)), by = .(cell_id)]
  # rast_nspp_catch <- map_to_mol(catch_spp_nspp, which = 'imp_nspp')
  # compareRaster(rast_nspp_all, rast_nspp_catch, values = TRUE)
}

### Process mean raster across taxa
if(!file.exists(impact_f_mean)) {
  message('... summarizing mean vulnerability map across all taxa...')
  catch_spp_mean <- process_mean_rasts(big_df)
  rast_mean <- map_to_mol(catch_spp_mean, which = 'imp_mean')
  message('Writing out mean raster: \n  ',
          str_replace(impact_f_mean, '/home/ohara/github/', 'GitHub:'))
  writeRaster(rast_mean, impact_f_mean, overwrite = TRUE)
}

### Process standard deviation raster across taxa using pooled var
if(!file.exists(impact_f_sdev)) {
  message('... summarizing standard deviation vulnerability map across all taxa...')
  ### break this into smaller chunks and parallelize over those?
  catch_spp_sdev <- process_sdev_rasts(big_df)
  rast_sdev <- map_to_mol(catch_spp_sdev, which = 'imp_sdev')
  message('Writing out std dev raster: \n  ',
          str_replace(impact_f_sdev, '/home/ohara/github/', 'GitHub:'))
  writeRaster(rast_sdev, impact_f_sdev, overwrite = TRUE)
}
```

```{r}
mean_rast_spp <- rast(impact_f_mean)
sdev_rast_spp <- rast(impact_f_sdev)
cv_rast_spp <- sdev_rast_spp / mean_rast_spp

map_cols <- hcl.colors(n = 50)

plot(log10(mean_rast_spp), col = map_cols, #zlim = c(0, 1),
     main = 'log_10(Mean) species mean impact: biomass removal',
     legend = TRUE, axes = FALSE)  
plot(cv_rast_spp, col = map_cols, 
     main = 'CV species impact: biomass removal',
     legend = TRUE, axes = FALSE)

```

## Impacts by FV-weighted mean vulnerability

Here we will rely on similar code to that used in script 3a_map_funct_entity_vulnerability.Rmd.

### Tidy the loop

Because this is a complex process, let's tidy the big `for` loop by breaking out key code as functions.

* `read_truncated_rangemap`: read in all range maps, truncating each one to just those cells in the current chunk.
* `calc_fe`: Calculate the functional vulnerability for a given functional entity based on the number of spp present.
* `calc_spp_cell_fe`: For each cell, identify all FEs and calculate the FV of each.  Because grouping by large numbers of groups (e.g, 100000 cells and multiple FEs), for crash-avoidance, this is parallelized. 
* `bind_maps_list`: for a list of truncated species maps, clean out NULL results and bind rows, keeping cell ID and species name.
* `calc_chunk_str_sum`: for a dataframe of truncated species maps

```{r helper functions}
read_truncated_rangemap <- function(f, chunk_start, chunk_end) {
  if(file.exists(f)) {
    df <- data.table::fread(f) %>%
      filter(between(cell_id, chunk_start, chunk_end)) %>%
      mutate(map_f = f)
    ### filter for presence and prob as needed
    if(str_detect(basename(f), 'am')) {
      df <- df %>%
        filter(prob >= .5) %>%
        select(-prob)
    } else {
      df <- df %>%
        filter(presence != 5) %>%
        select(-presence)
    }
  } else {
    df <- data.frame()
  }  
  if(nrow(df) == 0) {
    return(NULL) 
  } else {
    return(df)
  }
}

read_truncated_str_map <- function(f, chunk_start, chunk_end) {
  ### f <- catch_mapfile_fe_df$map_f[1000]
  if(!file.exists(f)) return(NULL)
  
  df <- data.table::fread(f) 
  if(nrow(df) > 0) {
    df <- df %>%
      filter(between(cell_id, chunk_start, chunk_end)) %>%
      mutate(rescaled_catch = as.numeric(rescaled_catch),
             map_f = f)
  } else {
    df <- NULL
  }  
  
  return(df)
}

bind_maps_list <- function(chunk_maps_list, spp_for_calc, join_col = 'map_f') {
  ### NOTE: for some reason, the bind_rows() in here sometimes causes
  ### unrecoverable errors when knitting, but seems OK when running chunks
  ### individually... try subbing with data.table::rbindlist 
  if(check_tryerror(chunk_maps_list)) {
    stop('Try-errors detected in bind_maps_list()!')
  }
  chunk_maps_bound <- chunk_maps_list %>%
    ### drop NULL instances (no spp cells - helps keep things from crashing)
    purrr::compact() %>% 
    data.table::rbindlist() %>%
    rename(!!join_col := map_f)
  
  if(nrow(chunk_maps_bound) > 0) {
    ### if no spp-cell data for this chunk, skip bind and return 0-length df
    chunk_maps_bound <- chunk_maps_bound %>%
      oharac::dt_join(spp_for_calc, by = join_col, type = 'left') %>%
      select(-all_of(join_col), -src) %>%
      distinct()
  }
  return(chunk_maps_bound)
}

calc_chunk_str_sum <- function(chunk_maps, spp_for_calc) {
  chunk_spp_vuln <- spp_for_calc %>%
    filter(species %in% chunk_maps$species) %>%
    select(species, v_score) %>%
    distinct()
  
  cell_id_df <- data.frame(cell_id = chunk_maps$cell_id %>% unique()) %>%
    mutate(cell_gp = rep(1:100, length.out = n()))
  cell_gps <- cell_id_df$cell_gp %>% unique()

  ### join map of catch scores to vuln scores; nontargeted have catch of 0,
  ### so fill NA scores with 0 also (to avoid NA problems)
  chunk_spp_str_vuln <- chunk_maps %>%
    oharac::dt_join(chunk_spp_vuln, by = 'species', type = 'left') %>%
    mutate(score = ifelse(is.na(v_score), 0, v_score))
  
  ### parallelize for speed! balance vectorization with parallel to reduce crashing...
  chunk_impact_list <- parallel::mclapply(cell_gps, mc.cores = 25,
          FUN = function(gp) { ### gp <- 41
            cell_ids <- cell_id_df %>% 
              filter(cell_gp == gp) %>% 
              .$cell_id
            df <- chunk_spp_str_vuln %>%
              filter(cell_id %in% cell_ids) %>%
              data.table() %>%
              .[ , ':='(impact = v_score * rescaled_catch)] %>%
              .[ , .(impact_mean = mean(impact),
                     impact_sd   = sd(impact), 
                     ### super-tiny fv (~ 1e-20) result in Inf var
                     fv = first(fv) %>% round(10)),
                 by = .(cell_id, fe_id)] %>%
              .[ , .(n_fe = length(unique(fe_id)),
                     fv_wt_mean_impact = Hmisc::wtd.mean(impact_mean, weights = fv),
                     fv_wt_sd_impact   = sqrt(Hmisc::wtd.var(impact_mean, weights = fv))),
                 by = .(cell_id)]
          })
  if(check_tryerror(chunk_impact_list)) {
    stop('Try-errors in calc_chunk_str_sum()!')
  }
  chunk_impact_sum_df <- data.table::rbindlist(chunk_impact_list)
  return(chunk_impact_sum_df)
}
```

```{r FV weighted impact by species iterating over chunks of cells}
chunk_size <- 100000
n_chunks <- ceiling(6.5e6 / chunk_size)
spp_fe <- spp_info_df %>% select(species, fe_id) %>% distinct()

tmp_stem_fe <- here_anx('impact_maps_by_taxon/impacts_tx_biomass_removal', 
                        'imp_fvwt_biomass_removal_chunk_%s_to_%s.tif')
### zxcv <- list.files(dirname(tmp_stem_fe), pattern = 'imp_fvwt', full.names = TRUE)
### unlink(zxcv)

for(chunk_i in 1:n_chunks) { 
  ### chunk_i <- 1
  ### chunk_i <- 65
  
  ### Set up chunk start and end and filenames; check whether maps 
  ### all stressors for this chunk...
  chunk_start <- (chunk_i - 1) * chunk_size + 1
  chunk_end   <- as.integer(chunk_i * chunk_size)
  chunk_text <- sprintf('chunk %s of %s (cells %s to %s)', 
                        chunk_i, n_chunks, chunk_start, chunk_end)

  ### check if chunk-stressor map is complete
  tmp_biomass_removal_fe <- sprintf(tmp_stem_fe, chunk_start, chunk_end)
  if(file.exists(tmp_biomass_removal_fe)) {
    message('Temp csv exists for ', chunk_text, ' for stressor biomass removal... skipping!')
    next()
  }
  
  ### If no chunk map, continue:
  ### Load species range and biomass removal pressure maps for this chunk, 
  ### then clean and bind:
  message('Loading ', nrow(range_mapfile_df), ' rangemaps cropped for ', chunk_text,  '...')
  chunk_rangemaps_list <- parallel::mclapply(range_mapfile_df$range_map_f, mc.cores = 40, 
                                        FUN = read_truncated_rangemap, 
                                        chunk_start = chunk_start, chunk_end = chunk_end) 
  chunk_rangemaps_raw <- bind_maps_list(chunk_maps_list = chunk_rangemaps_list, 
                                        spp_for_calc = range_mapfile_df,
                                        join_col = 'range_map_f')
  
  message('... Loading ', nrow(catch_mapfile_df), ' catch maps cropped for ', 
          chunk_text,  '...')
  chunk_catch_maps_list <- parallel::mclapply(catch_mapfile_df$catch_map_f, mc.cores = 40, 
                                        FUN = read_truncated_str_map, 
                                        chunk_start = chunk_start, chunk_end = chunk_end) 
  chunk_catch_maps_raw <- bind_maps_list(chunk_maps_list = chunk_catch_maps_list, 
                                         spp_for_calc = catch_mapfile_df,
                                         join_col = 'catch_map_f')
  
  ### Some error/edge case checking:
  if(nrow(chunk_catch_maps_raw) > 0 & nrow(chunk_rangemaps_raw) == 0) {
    ### Catch instances, but no species instance???
    stop('Non-zero catch but zero rangemaps... something ain\'t right!')
  }
  if(nrow(chunk_catch_maps_raw) == 0 & nrow(chunk_rangemaps_raw) == 0) {
    ### no catch, and no spp - write out empty file and move on
    warning('Neither catch nor species found for ', chunk_text,
            '... writing empty file...')
    write_csv(data.frame(), tmp_biomass_removal_fe)
    next()
  }
  if(nrow(chunk_catch_maps_raw) == 0 & nrow(chunk_rangemaps_raw) > 0) {
    ### no catch, but spp present - summarize here and save out
    message('... No catch found but ', n_distinct(chunk_rangemaps_raw$species),
            ' species found for ', chunk_text, '...')
    range_maps_fe <- calc_spp_cell_fe(spp_cells = chunk_rangemaps_raw,
                                      spp_fe = spp_fe)

    no_catch_df <- range_maps_fe %>%
      group_by(cell_id) %>%
      summarize(n_fe = n_distinct(fe_id), 
                fv_wt_mean_impact = 0, 
                fv_wt_sd_impact = 0)
    write_csv(no_catch_df, tmp_biomass_removal_fe)
    next()
  }

  
  catch_maps_filled <- chunk_rangemaps_raw %>%
    oharac::dt_join(chunk_catch_maps_raw, by = c('cell_id', 'species'), type = 'left') %>%
    mutate(rescaled_catch = ifelse(is.na(rescaled_catch), 0, rescaled_catch))

  ### OK, now we have species and cells for this chunk.  Calculate functional vulnerability!
  message('... Calculating functional vulnerability metrics for ', nrow(catch_maps_filled), 
          ' spp-cells in \n    ', chunk_text, '...')
  chunk_maps_fe <- calc_spp_cell_fe(spp_cells = catch_maps_filled,
                                    spp_fe = spp_fe)
  
  message('... In ', chunk_text,  ' rangemap dataframe: \n    ', nrow(chunk_maps_fe), 
          ' cell observations for ', n_distinct(chunk_maps_fe$species), ' species across ',
          n_distinct(chunk_maps_fe$fe_id), ' functional entities...')
  
  message('... Processing mean/sd vuln in ', chunk_text, ' to stressor: biomass removal...')
  chunk_str_sum <- calc_chunk_str_sum(chunk_maps = chunk_maps_fe, 
                                      spp_for_calc = spp_info_df)
  
  write_csv(chunk_str_sum, tmp_biomass_removal_fe)
}

```

### Aggregate chunk vulnerability maps to global map

For each stressor, pull in all chunk dataframes, assemble into dataframe, and save out as rasters.

```{r assemble chunk vuln maps to total maps}
rast_fe_out_stem <- here_anx('_output/impact_maps/impact_maps_funct_entity', 
                         'impact_fe_biomass_removal_%s.tif')

rast_mean_impact_fe_f <- sprintf(rast_fe_out_stem, 'mean')
rast_sdev_impact_fe_f <- sprintf(rast_fe_out_stem, 'sdev')

if(any(!file.exists(rast_mean_impact_fe_f, rast_sdev_impact_fe_f))) {
  message('Gathering impact chunk maps for biomass removal...')
  
  chunk_files <- list.files(dirname(tmp_stem_fe), 
                            pattern = 'imp_fvwt_biomass_removal_chunk', 
                            full.names = TRUE)

  impact_map_df <- parallel::mclapply(chunk_files, mc.cores = 33, 
                                      FUN = data.table::fread) %>%
    bind_rows()
  
  message('Converting impact maps to rasters for biomass removal...')
  mean_rast_fe <- map_to_mol(impact_map_df, by = 'cell_id', which = 'fv_wt_mean_impact')
  sdev_rast_fe <- map_to_mol(impact_map_df, by = 'cell_id', which = 'fv_wt_sd_impact')
  
  message('Writing impact rasters for biomass removal...')
  writeRaster(mean_rast_fe, rast_mean_impact_fe_f, overwrite = TRUE)
  writeRaster(sdev_rast_fe, rast_sdev_impact_fe_f, overwrite = TRUE)
  
  ### n_fe rast matches with that calculated in the vuln calcs:
  # n_fe_rast_main <- raster(here('_output/nspp_maps/n_fe_in_fvwt_vuln.tif'))
  # n_fe_rast_fvwt <- map_to_mol(impact_map_df, by = 'cell_id', which = 'n_fe')
  # compareRaster(n_fe_rast_main, n_fe_rast_fvwt, values = TRUE)
}
```

```{r}
mean_rast_fe <- rast(rast_mean_impact_fe_f)
sdev_rast_fe <- rast(rast_sdev_impact_fe_f)
cv_rast_fe <- sdev_rast_fe / mean_rast_fe

map_cols <- hcl.colors(n = 50)

plot(log10(mean_rast_fe), col = map_cols, #zlim = c(0, 1),
     main = 'log_10(Mean) FV-weighted impact: biomass removal',
     legend = TRUE, axes = FALSE)  
plot(cv_rast_fe, col = map_cols, 
     main = 'CV FV-weighted weighted impact: biomass removal',
     legend = TRUE, axes = FALSE)
```

## Difference maps

Loop over several focal stressors.  For each stressor, read in the maps of unweighted mean impact $\bar x$ and fv-weighted mean impact $\bar x_{FV}$.  Calculate difference as proportional difference in FV-weighted mean relative to unweighted mean impact: 
$$diff = (\bar x_{FV} - \bar x) / \bar x$$

```{r}
  
squish_rast <- function(r, qtile = .999) {
  r_vals <- values(r); r_vals <- r_vals[!is.na(r_vals)]
  r_zlim <- max(abs(quantile(r_vals, 1 - qtile)), abs(quantile(r_vals, qtile)))
  values(r)[values(r) > r_zlim] <- r_zlim
  values(r)[values(r) < -r_zlim] <- -r_zlim
  
  return(list(r = r, zlim = r_zlim))
}

### Set up rasters; trim extreme values
mean_diff_rast <- (mean_rast_fe - mean_rast_spp) / mean_rast_spp
mean_diff_rast[mean_rast_spp == 0] <- NA
mean_diff_squished <- squish_rast(mean_diff_rast, qtile = .999)
# cv_diff_rast <- (cv_rast_fvwt - cv_rast_unwt) / cv_rast_unwt
# cv_r_sq <- squish_rast(cv_diff_rast, qtile = .999)
high_diff_cells <- data.frame(spp = as.vector(values(mean_rast_spp)), 
                              fe  = as.vector(values(mean_rast_fe)),
                              cell_id = 1:ncell(mean_rast_spp)) %>%
  filter(!is.na(spp)) %>%
  mutate(diff = (fe - spp) / spp) %>%
  filter(diff == -1 | diff > 10) %>%
  filter(!is.infinite(diff)) %>%
  mutate(chunk = ceiling(cell_id / 1e5))
  
m_d_mask_0.10 <- m_d_mask_0.25 <- mean_diff_squished$r
values(m_d_mask_0.10)[values(mean_rast_spp) < .10 & values(mean_rast_fe) < .10] <- NA
values(m_d_mask_0.25)[values(mean_rast_spp) < .25 & values(mean_rast_fe) < .25] <- NA

message('Plotting difference maps for biomass removal...')

### Set up plot for diverging palette, and symmetric z limits around zero
map_cols <- hcl.colors(palette = 'Red-Green', n = 50, rev = TRUE)

plot(mean_diff_squished$r, col = map_cols, 
     main = '% Diff in mean vuln: biomass removal...',
     zlim = c(-mean_diff_squished$zlim, mean_diff_squished$zlim), 
     legend = TRUE, axes = FALSE)  
plot(m_d_mask_0.10, col = map_cols, 
     main = '% Diff in mean vuln masked 0.10: biomass removal...',
     zlim = c(-mean_diff_squished$zlim, mean_diff_squished$zlim), 
     legend = TRUE, axes = FALSE)  
plot(m_d_mask_0.25, col = map_cols, 
     main = '% Diff in mean vuln masked 0.25: biomass removal...',
     zlim = c(-mean_diff_squished$zlim, mean_diff_squished$zlim), 
     legend = TRUE, axes = FALSE)
```
