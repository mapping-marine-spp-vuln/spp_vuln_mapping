---
title: 'Process IUCN spp shapes to Mollweide'
author: "*Compiled on `r date()` by `r Sys.info()['user']`*"
output: 
  html_document:
    code_folding: hide
    toc: true
    toc_depth: 3
    toc_float: yes
    number_sections: true
    theme: cerulean
    highlight: haddock
    includes: 
      in_header: '~/github/src/templates/ohara_hdr.html'
  pdf_document:
    toc: true
---

``` {r setup, echo = TRUE, message = FALSE, warning = FALSE}

knitr::opts_chunk$set(fig.width = 6, fig.height = 4, fig.path = 'Figs/',
                      echo = TRUE, message = FALSE, warning = FALSE)

library(raster)
source('https://raw.githubusercontent.com/oharac/src/master/R/common.R')  ###
library(sf)

source(here('common_fxns.R'))
source(here('_setup/rasterize_fxns.R'))

dir_bli <- file.path(dir_M, 'git-annex/globalprep/_raw_data/birdlife_intl/d2019')
dir_shp <- file.path(dir_M, 'git-annex/globalprep/_raw_data/iucn_spp/d2020-1')
  ### These are shapefiles directly from IUCN as individual species map files
  ### (in the unzipped folder).
  ### in this folder are shapefiles at a taxonomic level.

```

# Summary

Using a set of IUCN species range maps, rasterize each species to 10 km x 10 km raster using `fasterize`.  Use `presence` field from shapefile.

* Subpopulation polygons must be identified and rasterized separately from the parent polygon; this must be done by sciname and subpop fields since the polygon IDs are based upon the parent ID.
* Regional assessments need not be determined at this stage - the ID numbers match the global ID numbers (including subpops).

# Data sources

* IUCN species shapefiles:  IUCN. (2020). The IUCN Red List of Threatened Species. Version 2020-1. Retrieved April 2020, from http://www.iucnredlist.org
* BirdLife International shapefiles: BirdLife International and Handbook of the Birds of the World. (2018). Bird species distribution maps of the world. Version 7.0. Available at http://datazone.birdlife.org/species/requestdis
* Bathymetry (GEBCO): Sandwell, D. T., Gille, S. T., & Smith, W. H. F. (2002, June). Bathymetry from Space:Oceanography, Geophysics, and Climate. Retrieved from https://www.gebco.net/


# Methods

## Read spp shapes, correct subpop IDs, `fasterize()`, depth clip, save to csv

We will loop over each species in each shapefile and rasterize separately, using `sf` and `fasterize` packages.  

* From the full map list, filter to a single shapefile
* Load shapefile using `st_read`, and correct subpop IDs from `shp_iucn_sid` to `iucn_sid`
* Loop over each `iucn_sid` in the shapefile, rasterizing (`fasterize()`) to 10 km^2 resolution, using "presence" field. 
    * clip to neritic (<=200 m) and shallow (<=60 m) depth raster if appropriate.  Otherwise mask to bathy raster.  Since bathy raster was created by masking to area raster, cells with any marine presence will be kept but any non-marine cells will be dropped.
    * Save as .tif and .csv, and compare average file sizes.  .csv easier to work with, but might be significantly larger than .tifs in the long run.
        * note: no longer saving as .tif for speed and file size - just use .csv instead!
    * use `mclapply()` to speed this up.
    
``` {r set up list of maps to rasterize}

reload <- FALSE

maps_to_rasterize <- read_csv(
    file.path(dir_data, sprintf('spp_marine_maps_%s.csv', api_version)),
    col_types = cols('subpop' = 'c')) %>%
  mutate(shp_file = str_replace(dbf_file, 'dbf$', 'shp'))

### rast_base for cell IDs
rast_base <- raster(here('_spatial/cell_id_mol.tif'))

### directory for mollweide rasters
dir_mol_rast <- file.path(dir_bd_anx, 'spp_rasts_mol_2020')
if(!dir.exists(dir_mol_rast)) dir.create(dir_mol_rast)

### If some species already processed, remove from the list to process.
if(reload == FALSE) {
  maps_already_rasterized <- list.files(dir_mol_rast,
                                        pattern = '.csv') %>%
    str_extract('[0-9]+') %>%
    as.integer()

  maps_to_rasterize <- maps_to_rasterize %>%
    filter(!iucn_sid %in% maps_already_rasterized)
}

### for selectively rerunning taxonomic groups
# maps_to_rasterize <- maps_to_rasterize %>%
#   filter(str_detect(dbf_file, 'REPTILES'))
# 
# asdf <- paste0('iucn_sid_', maps_to_rasterize$iucn_sid, '.csv')
# zxcv <- list.files(dir_mol_rast, 
#                    full.names = TRUE, 
#                    pattern = paste0(asdf, collapse = '|'))
# zzz <- file.mtime(zxcv)
# tmp_df <- data.frame(f = zxcv,
#                      t = zzz)
# 2018-09-12 12:38:31 check to make sure they're all later than this!
```

### Support functions to help in the processing of all these polygons

These are all stored in `rasterize_fxns.R`:

* `fix_fieldnames()` to adjust the shapefile field names so they match up
* `fix_turtle_polys()` to fix some gaps in turtle polygons before rasterizing
* `match_to_map()` to keep only the polygon features that still need to be rasterized, rather than keeping the overall shapefile 
* `clip_to_depth()` to take the resulting raster and mask the result to bathymetric rasters, based on several depth classes, e.g. neritic, shallow
* `buffer_tiny_polys()` to add a small buffer around extremely small polygons.  Some polygons are too small (e.g. < 100 km^2^, the size of a raster cell) to be picked up by the rasterization process and result in a zero range.  A small buffer will ensure some minimal inclusion for these highly endemic species.


``` {r rasterize and clip and save to csv}

if(nrow(maps_to_rasterize) == 0) { ### all maps accounted for as .csvs
  
  message('reload == ', reload, '... No maps to process...')
  
} else {
  
  message('reload == ', reload, '... Maps to process: ', nrow(maps_to_rasterize))

  ### These will be used as masks for clip_to_depth().  It's cheating
  ### to keep these in globalEnv but whatever for now.
  rast_bathy <- raster(file.path(dir_spatial,
                                 'bathy_mol.tif'))
  rast_neritic <- raster(file.path(dir_spatial,
                                   'bathy_mol_neritic.tif'))
  rast_shallow <- raster(file.path(dir_spatial,
                                   'bathy_mol_shallow.tif'))

  ################################################################.
  ### Loop over each distinct shapefile with species range maps
  ################################################################.
  shps <- maps_to_rasterize$shp_file %>% unique()
  for(i in seq_along(shps)) {
    ### i <- 2
    
    shp <- shps[i]
    
    message(i, ' of ', length(shps), ': reading ', basename(shp), ' from: \n  ', shp)

    if(!str_detect(shp, 'REPTILES')) {
      ### There's an issue with turtle polys - otherwise just read in the sf
      polys_all <- read_sf(shp, type = 6) %>%
        janitor::clean_names() %>%
        fix_fieldnames()
    } else {
      polys_all <- fix_turtle_polys(shp)
    }
      
    polys_match <- match_to_map(polys_all, maps_to_rasterize)
    
    ####################################################################.
    ### In each shapefile, loop over each species ID using mclapply().
    ####################################################################.
    
    spp_ids <- polys_match$iucn_sid %>% 
      sort() %>% 
      unique()
  
    message('Processing ', basename(shp), ' with ', length(spp_ids), ' species...')
    
    tmp <- parallel::mclapply(seq_along(spp_ids), mc.cores = 24, 
      FUN = function(j) { ### j <- 1    ### spp <- 178938
        spp <- spp_ids[j]
        message(j, ' of ', length(spp_ids), ': Processing ', spp, ' in ', 
                basename(shp), ' (group ', i, ' of ', length(shps), ')...\n')
        
        spp_shp <- polys_match %>%
          filter(iucn_sid == spp)
        
        spp_shp <- spp_shp %>%
          valid_check() %>%
            ### if invalid geom, and bounds exceeded, buffer to 0
          clip_to_globe() %>%
            ### make sure bounds don't exceed +/- 180
          smoothr::densify(max_distance = 0.5) %>%
            ### add points to ensure smooth transform to Mollweide
          st_transform(crs(rast_base)) %>%
            ### transform to Mollweide ~10km x 10km resolution
          buffer_tiny_polys()
            ### identify tiny polys, buffer to ensure some representation
        
        spp_rast <- fasterize::fasterize(spp_shp, rast_base, 
                                         field = 'presence', fun = 'min')
        
        ### clip the rasterized polygons to various depth regimes
        ### depending on species characteristics
        spp_rast <- clip_to_depth(spp_rast, spp_shp)
        
        ### convert to dataframe and write out as a csv:
        spp_present <- data.frame(cell_id  = values(rast_base),
                                  presence = values(spp_rast)) %>%
          filter(!is.na(presence))
        
        if(nrow(spp_present) == 0) {
          message('Species ID ', spp, ' resulted in a zero-length dataframe.')
        }
        
        write_csv(spp_present, file.path(dir_mol_rast,
                                         sprintf('iucn_sid_%s.csv', spp)))
        
        return(NULL)
      }) ### end of mclapply FUN definition
  } ### end of for loop over each species group
} ### end of "if" check to make sure there are any maps to rasterize

```

``` {r file size testing}

maps <- read_csv(
    file.path(dir_data, sprintf('spp_marine_maps_%s.csv', api_version)),
    col_types = cols('subpop' = 'c')
  ) %>%
  select(spp_group = dbf_file, iucn_sid, sciname, subpop) %>%
  mutate(spp_group = str_replace(spp_group, '.+_raw_data/|.dbf', '')) %>%
  distinct()

csvs <- list.files(dir_mol_rast) %>%
  str_extract('[0-9]+')

x <- list.files(dir_mol_rast,
                full.names = TRUE)
x <- x[str_detect(x, paste0(csvs, collapse = '|'))]

y <- data.frame(size = file.size(x)) %>%
  mutate(f = x,
         iucn_sid = str_extract(basename(f), '[0-9]+'),
         iucn_sid = as.integer(iucn_sid)) %>%
  left_join(maps, by = 'iucn_sid')

z <- y %>%
  group_by(spp_group) %>%
  summarize(mean_size_kb = round(mean(size) / 1024), 
            n_spp = n()) %>%
  # mutate(ratio = tif / csv) %>%
  ungroup() %>%
  arrange(desc(mean_size_kb))

# sum(z$tif * z$n_spp) ### 8,146,818,040:  8.1 GB for 5369 spp (pre depth clip) (some dupes, oops)
# sum(z$csv * z$n_spp) ### 3,179,096,927:  3.2 GB for 5369 spp (pre depth clip) (some dupes, oops)
# sum(z$csv * z$n_spp) ### 2,218,041,470:  2.2 GB for 5332 spp (post depth clip) (dupes removed!)

zz <- y %>%
  summarize(mean_size_kb = round(mean(size) / 1024), 
            n_spp = n()) %>%
  mutate(spp_group  = 'TOTAL: all available spp maps',
         total_size_kb = round(sum(y$size) / 1024)) %>%
  bind_rows(z) %>%
  select(spp_group, n_spp, everything())

DT::datatable(zz, caption = 'Mean file size by group and type')

```


