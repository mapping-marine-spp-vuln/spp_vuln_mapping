---
title: "Unsupervised Random Forest on Species Traits"
author: "Casey O'Hara"
date: "6/25/2021"
output: html_document
---

``` {r setup, echo = TRUE, message = FALSE, warning = FALSE}

knitr::opts_chunk$set(echo = TRUE, message = FALSE, warning = FALSE)

library(oharac)
oharac::setup()
library(randomForest)
library(caret)
library(cluster)
library(factoextra)
library(ggfortify)
source(here('fb_slb_fxns.R'))
```

Adapted from `http://gradientdescending.com/unsupervised-random-forest-example/`


```{r}
set.seed(42)

gf_cut <- 3

trait_length <- read_csv(here('_data/traits_grouping/trait_length.csv')) %>%
  filter(gf_level <= gf_cut) %>%
  select(species, log_l = value)
trait_reprod <- read_csv(here('_data/traits_grouping/trait_reproduction_raw.csv')) %>%
  filter(gf_level <= gf_cut) %>%
  select(species, depend_score, log_gen_time, log_max_fecund, pld_score, protect_score)
trait_depth  <- read_csv(here('_data/traits_grouping/trait_depth_raw.csv')) %>%
  filter(gf_level <= gf_cut) %>%
  select(species, depth, terr_score, vert_mig_score)
trait_mobil  <- read_csv(here('_data/traits_grouping/trait_mobility.csv')) %>%
  filter(gf_level <= gf_cut) %>%
  select(species, mob_score = value)
trait_troph  <- read_csv(here('_data/traits_grouping/trait_trophic_level.csv')) %>%
  filter(gf_level <= gf_cut) %>%
  select(species, troph_level = value)
traits_df <- trait_length %>%
  full_join(trait_reprod) %>%
  full_join(trait_depth) %>%
  full_join(trait_mobil) %>%
  full_join(trait_troph)

```

# Supervised Random Forest

Let's try a supervised random forest to classify into some taxonomic level, e.g., class?

```{r attach taxa}
taxa <- read_csv(here('_data/vuln_data/vuln_gapfilled_tx.csv')) %>%
  select(class:species) 
  
traits_dropna <- traits_df %>%
  left_join(taxa, by = 'species') %>%
  drop_na()
### leaves only 17008 observations...
```

``` {r split into training and validation}
### split training and validation sets
holdout <- sample(1:nrow(traits_dropna), nrow(traits_dropna)/3, replace = FALSE)

valid <- traits_dropna[holdout, ]
train <- traits_dropna[-holdout, ] # %>% sample_n(1000)
```

Because we have nine predictor variables, select `mtry = 3` at a time ($\sqrt{\text{# of traits}}$).  Note randomForest seems to scale ~ $n^{1.9}$ (on my laptop - but about the same on Mazu)
``` {r supervised random forest}
### for ntree = 1000:
###     n =  1000, t =   1.981
###     n =  2000, t =   6.886
###     n =  3000, t =  14.739
###     n =  4000, t =  25.421
###     n =  5000, t =  40.262
###     n =  8000, t = 106.578
###     n = 11339, t = 199.23
n <- c(1000,  2000,  3000, 4000, 5000, 8000, 11339)
t <- c(1.981, 6.886, 14.739, 25.421, 40.262, 106.578, 199.23)
power_fit <- lm(log(t) ~ log(n))
### slope = power
df <- data.frame(n, t) %>%
  mutate(ln_fit = 1.981*(n/1000)^1.9)
ggplot(df, aes(x = n, y = t)) +
  geom_point() +
  geom_line(aes(y = ln_fit), color = 'red') +
  labs(x = 'number of spp obs', y = 'RF processing time (s)')

system.time({
  sup_rf_class <- randomForest(x = train %>% 
                                 select(where(is.numeric)), 
                               y = factor(train$class), ### dependent must be factor 
                               mtry = 3, ntree = 1000, proximity = TRUE)
})
sup_rf_class
varImpPlot(sup_rf_class)
```

``` {r use supervised RF results to predict class on validation set}
# predict on test set
class_predicted <- predict(sup_rf_class, valid %>% select(where(is.numeric)))
valid_df <- data.frame(orig = valid$class, pred = class_predicted)

confusionMatrix(table(valid_df$orig, valid_df$pred))
```

Supervised RF seems to do a great job of predicting a species' class based on the traits given.  What will happen when we run an unsupervised RF to inform a clustering algorithm?

# Unsupervised Random Forest

Let's apply the process to the traits dataframe, without supervision, to create a proximity matrix.  NOTE: This seems about half the speed of the supervised?

```{r}
### n = 1000, t = 3.663; n = 2000, t = 10.096; n = 17008, t = 477.686
system.time({
  unsup_rf <- randomForest(x = traits_dropna %>% 
                             # sample_n(2000) %>%
                             select(where(is.numeric)),
                           mtry = 3, ntree = 1000, proximity = TRUE)
})
unsup_rf
varImpPlot(unsup_rf)
```

## identify an appropriate number of clusters

### Knee method

Iterate over different values of $k$ and identify a "knee".  When looking for a large number of clusters, however, it seems like this method is not super useful...
``` {r knee method, eval = FALSE}
set.seed(42)
wss <- function(k, mtx) {
  # cluster::pam(mtx, k, diss = TRUE)
  kmeans(mtx, k, nstart = 10)$tot.withinss
}

prox_mtx <- unsup_rf$proximity
prox_mtx1 <- prox_mtx[1:1000, 1:1000]

dist_mtx <- as.dist(1 - prox_mtx)
dist_mtx1 <- as.dist(1 - prox_mtx1)

# Compute and plot wss for a range of k
k_vec <- 1:20
wss_values <- parallel::mclapply(k_vec, wss, mtx = dist_mtx, mc.cores = 5) %>%
  unlist()

plot(k_vec, wss_values,
     type="b", pch = 19, frame = FALSE, 
     xlab="Number of clusters K",
     ylab="Total within-clusters sum of squares")

plot(k_vec[9:20], wss_values[9:20],
     type="b", pch = 19, frame = FALSE, 
     xlab="Number of clusters K",
     ylab="Total within-clusters sum of squares")

```


#### Silhouette method 

From https://uc-r.github.io/kmeans_clustering:

> In short, the average silhouette approach measures the quality of a clustering. That is, it determines how well each object lies within its cluster. A high average silhouette width indicates a good clustering. The average silhouette method computes the average silhouette of observations for different values of _k_. The optimal number of clusters _k_ is the one that maximizes the average silhouette over a range of possible values for _k_.

Because this is very time consuming, consider using just a sample of the overall dataset to determine optimal number of clusters, and cache the results.

```{r silhouette method}
set.seed(42)
avg_sil <- function(k, df) {
  km_res <- kmeans(df, centers = k, nstart = 4)
  ss <- silhouette(km_res$cluster, dist(df))
  mean(ss[, 3])
}

df_resc_samp <- df_rescale[sample(1:nrow(df_rescale), size = 24000, replace = FALSE), ]

# extract avg silhouette for 6-20 clusters
silh_file <- sprintf(here('tmp/silhouette_%s.csv'), nrow(df_resc_samp))
if(!file.exists(silh_file)) {
  message('Calculating silhouette for ', nrow(df_resc_samp), ' observations.')
  # Compute and plot wss for k = 6 to k = 20
  k_vec <- 6:20
  avg_sil_values <- sapply(k_vec, avg_sil, df = df_resc_samp)
  
  silh_df <- data.frame(k = k_vec, silh = avg_sil_values)
  write_csv(silh_df, silh_file)
}
silh_df <- read_csv(silh_file)

plot(silh_df$k, silh_df$silh,
     type = "b", pch = 19, frame = FALSE, 
     xlab = "Number of clusters K",
     ylab = "Average Silhouettes")

# Similar to the elbow method, this process to compute the 'average 
# silhoutte method' has been wrapped up in a single function (fviz_nbclust):
silh_plot_file <- sprintf(here('tmp/silh_plot_%s.png'), nrow(df_resc_samp))
if(!file.exists(silh_plot_file)) { 
  message('Plotting optimal clusters by silhouette method for ', nrow(df_resc_samp), ' observations.')
  # unlink(silh_plot_file)
  x <- fviz_nbclust(df_resc_samp, kmeans, k.max = 20, method = "silhouette")
  ggsave(plot = x, filename = silh_plot_file)
}
knitr::include_graphics(silh_plot_file)

```

#### Gap Statistic Method

From https://uc-r.github.io/kmeans_clustering: 

> The gap statistic has been published by R. Tibshirani, G. Walther, and T. Hastie (Standford University, 2001). The approach can be applied to any clustering method (i.e. K-means clustering, hierarchical clustering). The gap statistic compares the total intracluster variation for different values of k with their expected values under null reference distribution of the data (i.e. a distribution with no obvious clustering). The reference dataset is generated using Monte Carlo simulations of the sampling process. That is, for each variable
$x_i$ in the data set we compute its range $[\min(x_i), \max(x_i)]$ and generate values for the $n$ points uniformly from the interval min to max.

> For the observed data and the the reference data, the total intracluster variation is computed using different values of $k$. The gap statistic for a given $k$ is defined as follow:

$$Gap_n(k) = E^*_n \log(W_k) - \log(W_k)$$
>where $E^*_n$ denotes the expectation under a sample size $n$ from the reference distribution. $E^*_n$ is defined via bootstrapping (B) by generating B copies of the reference datasets and, by computing the average $\log(W^*_k)$. The gap statistic measures the deviation of the observed $W_k$ value from its expected value under the null hypothesis. The estimate of the optimal clusters ($\hat{k}$) will be the value that maximizes $Gap_n(k)$. This means that the clustering structure is far away from the uniform distribution of points.

```{r gap stat method}
set.seed(42)
gap_stat_plot_file <- sprintf(here('tmp/gap_stat_%s.png'), nrow(df_resc_samp))
if(!file.exists(gap_stat_plot_file)) {
  gap_stat <- clusGap(df_resc_samp, verbose = FALSE,
                      FUN = kmeans, nstart = 25,
                      K.max = 20, B = 50)
  # Print the result
  print(gap_stat, method = "firstmax")
  
  x <- fviz_gap_stat(gap_stat)
  ggsave(plot = x, filename = gap_stat_plot_file)
}
knitr::include_graphics(gap_stat_plot_file)
```

This method seems to indicate that more clusters is better all the way to 15.

<!-- ### Distance matrix?  -->
``` {r distance matrix, eval = FALSE}
rowsamps <- sample(1:nrow(df_rescale), 500, replace = FALSE)
d_mtx_samp <- get_dist(df_rescale[rowsamps, ])
d_mtx <- get_dist(df_rescale)
fviz_dist(d_mtx_samp, 
          gradient = list(low = "#00AFBB", mid = "white", high = "#FC4E07"),
          show_labels = FALSE)

```


Now the unsupervised random forest model is fit weâ€™ll extract the proximity matrix and use this as input to a PAM procedure.  Based on prior analysis, let's try group into k = 15.  

``` {r}
prox_mtx <- unsup_rf$proximity

pam_rf <- pam(prox_mtx, k = 3)
pred <- data.frame(clust   = pam_rf$clustering, 
                   species = penguins_clean$species)
table(pred)
#      species
# clust Adelie Chinstrap Gentoo
#     1    151        68     12
#     2      0         0     52
#     3      0         0     59

plot_df <- penguins_clean %>%
  mutate(clust = factor(pam_rf$clustering))
ggplot(plot_df, aes(x = flipper_length_mm, y = bill_length_mm, col = clust, pch = species)) + 
  geom_point(size = 3) + 
  scale_color_manual(values = myColRamp(3))
```
 

Lots of misclassifications on the unsupervised, since Adelie and Chinstrap are pretty similar overall - Gentoo are split, while Adelie and Chinstrap are all grouped into one group with a few Gentoo

# Comparison with straight kmeans and PAM

A standard kmeans and PAM procedure will be fit for comparison.

## straight kmeans

```{r}
peng_scale <- scale(penguins_clean %>% select(ends_with(c('_mm', '_g'))))
km <- kmeans(peng_scale, 3, nstart = 200)
pred_km <- data.frame(clust = km$cluster, 
                      species = penguins_clean$species)
table(pred_km)
#      species
# clust Adelie Chinstrap Gentoo
#     1    127         5      0
#     2     24        63      0
#     3      0         0    123

plot_df <- penguins_clean %>%
  mutate(clust = factor(km$cluster))
ggplot(plot_df, aes(x = flipper_length_mm, y = bill_length_mm, col = clust, pch = species)) + 
  geom_point(size = 3) + 
  scale_color_manual(values = myColRamp(3))
```
Straight k-means does a pretty good job, identifying all Gentoo as a single group, and reasonably separating Adelie and Chinstrap into different groups.

```{r}
pm <- pam(peng_scale, 3)
pred_pm <- data.frame(clust = pm$clustering, 
                      species = penguins_clean$species)
table(pred_pm)
#      species
# clust Adelie Chinstrap Gentoo
#     1    124         5      0
#     2     27        63      0
#     3      0         0    123

plot_df <- penguins_clean %>%
  mutate(clust = factor(pm$cluster))
ggplot(plot_df, aes(x = flipper_length_mm, y = bill_length_mm, col = clust, pch = species)) + 
  geom_point(size = 3) + 
  scale_color_manual(values = myColRamp(3))
```
Straight PAM also does a reasonable job of differentiating into reasonable classes.

Based on this, is unsupervised RF clustering really all that great?...


